portfolio-harness-system {
    include "common"

    portfolio {
        # The port to run the http server on
        http-port=<%=@http_port%>
        # This is the path to the plugins (defaults to "plugins")
        plugin-path="plugins"
        
        # Proxy properties. Leave this section blank if you dont need proxy support.
        <% if @auth_host.to_s != '' && @proxy_host.to_s != '' %>
        easy.proxy.1 = "<%= @auth_host %> -> <%= @proxy_host %>"
        <% end %>

        easy.proxy.1=""

        # This section is for monitoring the application
        monitoring {
            # What is the application name (used for persisting metrics)
            application.name = "Portfolio Harness"
            # the prefix to append metrics being sent to graphite
            metric.prefix = "<%= node['wt_monitoring']['metric_prefix'] %>"
            # Are we monitoring at all
            enabled = <%= node['wt_monitoring']['monitoring_enabled'] %>
            # should we publish to jmx
            jmx.enabled = <%= node['wt_monitoring']['jmx_enabled'] %>
            # the prefix to append metrics being sent to graphite
            jmx.port = <%= @jmx_port %>
            # Are metrics going to graphite
            graphite.enabled = <%= @graphite_enabled %>
            # What is the host of graphite
            graphite.host="<%= node[:wt_monitoring][:graphite_server] %>"
            # What port is graphite listening on
            graphite.port=<%= node[:wt_monitoring][:graphite_port] %>
            # How often (minutes) should we flush metrics to graphite
            graphite.interval=<%= @graphite_interval %>
            # Should we include the JVM metrics when sending to graphite
            graphite.vmmetrics=<%= @graphite_vmmetrics %>
            # This is a regular expression for which metrics should be sent on to graphite. All metrics are still exposed via JMX or the metrics endpoint
            graphite.regex="<%= @graphite_regex %>"
            # Are health checks enabled
            healthcheck.enabled=<%= node['wt_monitoring']['healthcheck_enabled'] %>
        }

        # This section is for connecting to Zookeeper
        zookeeper {
            # The data center to point to 
            datacenter="<%= @datacenter %>"
            # The environment within the center
            pod="<%= @pod %>"
            # The list of fqdn to the zookeeper quorom. Example: hzoo01.staging.dmz,hzoo02.staging.dmz,hzoo03.staging.dmz.
            quorum="<%= @zookeeper_quorum  %>"
            # The zookeeper session timeout. Defaults to 30,000 milliseconds.
            session.timeout.ms=30000
            # The alloted time to try an connect to zookeeper. Defaults to 30,000 miliseconds.
            connection.timeout.ms=30000
            # The alloted time to sleep before trying to connect to zookeeper. Defaults to 5,000 miliseconds.
            retry.sleep.ms=5000
            # The number of times to retry to connect to zookeeper. Defaults to 150.
            retry.count=150
        }

        # Authorization
        authentication {
            cam {
                # This is the host for the CAM service
                host="<%=@cam_host%>"
                # This is the port for the CAM service
                port=<%=@cam_port%>
                # This is the url to the REST service for getting a token's detailed information
                context.url="/context/%s/scope/%s"
            }
        }
       
        #temporary placement for these
        discovery {
            sapi {
                zookeeer-root ="/StreamingZMQ"
                node-path="/sapi/node"
                account-counters-format="/sapi/counters/%s/account"
            }
        }
    }

    akka {
        event-handlers = ["com.webtrends.portfolio.logging.Logger"]
        loglevel=DEBUG
        log-config-on-start = off
        receive=on

        debug {
            receive = on
            autoreceive = on
            lifecycle = on
            event-stream = on
        }

        actor {
            serializers {
                proto = "akka.remote.serialization.ProtobufSerializer"
                java = "akka.serialization.JavaSerializer"
                bytes = "akka.serialization.ByteArraySerializer"
                blank = "akka.serialization.NullSerializer"
            }

            serialization-bindings {
                "[B" = bytes
            }

            provider = "akka.remote.RemoteActorRefProvider"
            deployment {
                /system/processor {
                    router = round-robin
                    nr-of-instances = 3
                }
                /system/http-base {
                    router = round-robin
                    nr-of-instances = 3
                }
            }

            debug {
                autorecieve=on
            }
        }
        remote {
            log-sent-messages = on
            log-received-messages = on
            transport = "akka.remote.netty.NettyRemoteTransport"

            netty {
                hostname = "<%= node[:fqdn] %>"
                port = 2552
                write-buffer-high-water-mark = 1048576b
                write-buffer-low-water-mark = 0b
                send-buffer-size = 1048576b
                receive-buffer-size = 1048576b
            }
        }

        io {
            # In bytes, the size of the shared read buffer. In the span 0b..2GiB.
            read-buffer-size = 32KiB
            # Specifies how many ops are done between every descriptor selection
            select-interval = 100
            # Number of connections that are allowed in the backlog.
            # 0 or negative means that the platform default will be used.
            default-backlog = 1000
        }
    }

    spray {
        io {
            #parallelism=2
            #no-delay=1
        }
        can {
            server {
                remote-address-header=<%=@remote_address_hdr%>
                request-timeout=5s
		parsing {
			max-uri-length=<%=@max_uri_length%>
		}
            }
        }
    }
 }
